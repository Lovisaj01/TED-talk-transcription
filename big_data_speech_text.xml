<?xml version="1.0" encoding="UTF-8"?>
<?xml-model href="http://www.tei-c.org/release/xml/tei/custom/schema/relaxng/tei_all.rng" type="application/xml" schematypens="http://relaxng.org/ns/structure/1.0"?>
<?xml-model href="http://www.tei-c.org/release/xml/tei/custom/schema/relaxng/tei_all.rng" type="application/xml"
	schematypens="http://purl.oclc.org/dsdl/schematron"?>
<?xml-model href="https://tei-c.org/release/xml/tei/custom/schema/xsd/tei_all.xsd" type="application/xml-xsd"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0">
  <teiHeader><!--Metadata-->
      <fileDesc>
         <titleStmt>
            <title>Title</title>
         </titleStmt>
         <publicationStmt>
            <p>Publication Information</p>
         </publicationStmt>
         <sourceDesc>
            <scriptStmt xml:id="CNN12">
             <bibl>
              <author>Cathy O'Neil</author>
              <title>The era of blind faith in big data must end</title>
              <date when="2017-04">April 2017</date>
             </bibl>
            </scriptStmt>
           </sourceDesc>
      </fileDesc>
  </teiHeader>
  <text>
    <body>
      <head>The era of blind faith in big data must end</head>
      <listPerson>
         <person xml:id="Cathy"></person>
         <person xml:id="Audience"></person>
      </listPerson>
      <u who="#Cathy">
         Algorithms are everywhere. <pause/>
         They sort and separate the winners from the losers. 
         The winners get the job, or a good credit card offer. 
         The losers don’t even get an interview, or they pay more for insurance. 
         We’re being scored with secret formulas <kinesic><desc>nodding head</desc></kinesic>that we don’t understand that often don’t have systems of appeal. 
         That begs the question: What if the algorithms <pause/> are wrong? 
      </u>
      <u>
         To build an algorithm, you need two things, <kinesic><desc>hand gesture for on the one hand</desc></kinesic> you need data, what happened in the past, <kinesic><desc>hand gesture for on the other hand</desc></kinesic>and a definition of success, the thing you’re looking for and often hoping for.  
      </u>
      <u>
         You train <kinesic><desc>arm motion for training</desc></kinesic>
         an algorithm by <del>looking</del> <kinesic><desc>hand gesture for process of understanding </desc></kinesic>
         figuring out, for the algorithm figures out what is <kinesic><desc>hand gesture for moving one thing from one side to another</desc></kinesic>
         associated with success, what situation <kinesic><desc>hand gesture for moving one thing from one side to another</desc></kinesic>leads to success. 
      </u>
      <u>
         Actually, <kinesic><desc>hand gesture to calm down the audience</desc></kinesic>
         everyone uses algorithms, they just don’t formalize them in written code. 
         Let me give an example <kinesic><desc>gesturing towards the audience</desc></kinesic>: 
         <kinesic><desc>gestures towards herself</desc></kinesic>I use an algorithm every day to make 
         a meal for my family. <kinesic><desc>hand gesture for grabbing something</desc></kinesic>
         The data I use <kinesic><desc>hand gesture for listing</desc></kinesic> is the 
         <list>
            <item>ingredients in my kitchen, </item>
            <item>the time I have, </item>
            <item>the ambition I have</item>
         </list>, and <kinesic><desc>hand gesture for no don’t suppose I forgot</desc></kinesic> I curate that data. 
         <kinesic><desc>hand gesture for no</desc></kinesic>I don’t count <kinesic><desc>symbolizing a package</desc></kinesic>
         those little packages of ramen noodles as food. <vocal who="#Audience"><desc>laughter</desc></vocal>
      </u>
      <u>
         My definition of success is, a meal of success is my kids eat vegetables. It’s very different 
         from if my youngest son were in charge, he’d say if he gets to eat lots of Nutella. 
         <vocal who="#Audience"><desc>laughter</desc></vocal> But I get to choose success. I am in charge. 
         My opinion matters. That’s the first rule of algorithms: algorithms are opinions embedded in code. 
         It’s very different from what you think most people think of algorithms. 
         <del>They think of algorithms </del>, they think algorithms are objective and true and scientific. 
         That’s a marketing trick <vocal who="#Audience"><desc>a slight "a-ha"</desc></vocal>. 
         It’s also a marketing trick to intimidate you with algorithms: 
         to make you trust and fear math and algorithms because you trust and fear mathematics. 
         A lot can go wrong when we put blind faith in big data. <pause />
      </u>
      <u>
         This is Kiri Soares. She’s a high school principal in Brooklyn. In twenty eleven, 
         she told me her teachers were being scored with a complex, secret algorithm 
         called a “value-added model”. I told her, “Well, figure out what the formula is, show it to me. 
         <kinesic><desc>nodding head</desc></kinesic>I’m gonna explain it to you.” She said, 
         “Well, I tried to get the formula, but my department of education contact told me it was 
         <quote>math</quote> and I wouldn’t understand it. <pause /> 
         <kinesic><desc>finger raised</desc></kinesic>It gets worse. 
         The New York Post filed a Freedom of Information Act request. Got all the teachers’ names, 
         and all their scores and they publish them as an act of teacher-shaming. 
         When I tried to get the formulas of source code, through the same means, I was told I couldn’t. 
         I was denied. <kinesic><desc>fingers raised</desc></kinesic>I later found out, 
         that nobody in New York had access to that formula. No one understood it. 
         Then someone really smart got involved, Gary Rubenstein. 
         He found six hundred and sixty-five teachers <kinesic><desc>connection to before mentioned</desc></kinesic>
         from that New York Post data that actually had two scores. 
         That could happen if they were teaching seventh grade math and eighth grade math. 
         He decided to plot them. Each dot represents a teacher. 
         <kinesic><desc>shows a diagram with a swarm of unstructured dots</desc></kinesic>
         <vocal who="#Audience"><desc>laughter</desc></vocal>
      </u>
      <u>
         What is that? <vocal who="#Audience"><desc>laughter</desc></vocal> 
         That should never have been used for individual assessment. It’s almost a random number generator. 
         <vocal who="#Audience"><desc>applause</desc></vocal>But it was. 
         This is Sarah Wysocki. She got fired, along with two hundred and five other teachers from the Washington DC school district, 
         even though she had great recommendations from her principal and the parents of her kids. 
      </u>
      <u>
         I know what a lot of you guys are thinking, especially data scientists - the AI experts here. 
         You’re thinking, “Well, I would never make an algorithm that inconsistent”. 
         But algorithms can go wrong, even have deeply destructive effects with good intentions. 
         And, whereas an airplane that’s designed badly crashes to the earth and everyone sees it, 
         an algorithm designed badly <pause /> can go on for a long time, silently wreaking havoc.  
      </u>
      <u>
         This is Roger Ailes. <vocal who="#Audience"><desc>laughter</desc></vocal><kinesic><desc>nodding head</desc></kinesic> 
         He founded Fox News in nineteen ninety-six. More than twenty women complained about sexual harassment. 
         They said they weren’t allowed to succeed at Fox News. He was ousted last year, but we’ve seen recently that the problems have persisted. 
         That begs the question: What should Fox News do to turn over another leaf? 
         Well, what if they replaced their hiring process with a machine-learning algorithm? That sounds good, right? 
         Think about it. The data, what would the data be? 
         A reasonable choice would be the last twenty-one years of applications to Fox News. Reasonable. 
         What about the definition of success? Reasonable choice would be, well, who is successful at Fox News? 
         I guess someone who, say, stayed there for four years and was promoted at least once. Sounds reasonable. 
         And then, the algorithm would be trained. 
         It would be trained to look for people to learn what led to success, what kind of applications 
         <del>led to</del> historically led to success, by that definition.  
      </u>
      <u>
         <kinesic><desc>finger raised</desc></kinesic>Now think of what would happen if we applied that to a current pool of applicants. 
         It would filter out women <pause /> because they do not look like people who were successful in the past.  
      </u>
      <u>
         Algorithms don’t make things fair if you just blithely, blindly apply algorithms. 
         They don’t make things fair. They repeat our past practices, our patterns. They automate the status quo. 
         That would be great if we had a perfect world, but we don’t. 
         <kinesic><desc>finger raised</desc></kinesic>And I'll add that most companies don’t have embarrassing lawsuits. 
         But the data scientist in those companies are told <kinesic><desc>hand gesture for strictly</desc></kinesic>to follow the data, 
         <kinesic><desc>hand gesture for strictly</desc></kinesic>to focus on accuracy. 
         Think about what that means. Because we all have bias, it means they could be codifying <kinesic><desc>hand gesture for first of a number of reasons</desc></kinesic>sexism or any other kind of bigotry. 
      </u>
      <u>
         Thought experiment, because I like them: <vocal><desc>inhale</desc></vocal>
         an entirely segregated society – racially segregated, all towns, all neighborhoods, 
         and where we send the police only to the minority neighborhoods to look for crime. 
         The arrest data would be very biased. What if, on top of that, 
         we found the data scientist and paid the data scientist to predict where the next crime would occur? 
         <kinesic><desc>hand gesture for ironically no clue or obvious</desc></kinesic>Minority neighborhoods. 
         Oh, or to predict who the next criminal would be? A minority. 
      </u>
      <u>
         The data scientists would brag about how great and how accurate their model would be, 
         and they’d be <kinesic><desc>nodding head</desc></kinesic>right. 
      </u>

      <!-- time 07:07 -->
      <u>
         <kinesic><desc>expressive hands movement</desc></kinesic>Now reality isn't that drastic, but we do have severe segregations in many cities and towns.
         We have plenty of evidence <pause/> of bias pleasing and justice system data.
         <kinesic><desc>left hand index finger raised up</desc></kinesic> And we actually do <kinesic><desc>left hand index and thumb fingers do a circle</desc></kinesic> predict hot spots, places where crimes would occur.
         And we do predict, in fact, the individual <incident><desc>cri</desc></incident> criminality.
         The criminality of individuals.
         The news organization ProPublica recently looked into one of those recidivism risk algorithms, as they're called, being used in Florida during sentencing <pause/> by judges.
         Bernard, on the left, <kinesic><desc>turns to the projected pictures and points to the left one</desc></kinesic> the black man, was scored ten out of ten.
         Dylan, on the right, three out of ten.
         Ten out of ten, high risk.
         Three out of ten, low risk.
         <kinesic><desc>expressive hands movement</desc></kinesic>They were both brought in for drug possession.
         They both had records <kinesic><desc>points to the picture on the right</desc></kinesic> but Dylan <kinesic><desc>points to the picture on the right</desc></kinesic> <pause/> had a felony <kinesic><desc>points to the picture on the left</desc></kinesic> but Bernard didn't.
         <kinesic><desc>expressive hands movement</desc></kinesic>This matters because the higher score you are, the more likely you're being given a longer sentence.<pause/>
         <shilft new=asc/>What's going on? 
         Data laundering.
         <vocal who="#Audience"><desc>aha</desc></vocal>
         It's a process by which technologists hide ugly truths inside black box algorithms and call them objective.
         Call them meritocratic. <pause/>
         When they're secret, important and destructive, I've coined a term for these algorithms: weapons of math destruction.
         <vocal who="#Audience"><desc>laughter, cheer</desc></vocal> <kinesic who="#Audience"=><desc>applause</desc></kinesic>
      </u>
      <u>
         <kinesic><desc>expressive hands movement</desc>They're everywhere<shilft new=asc/>and it's not a mistake! 
         These are private companies building private algorithms for private ends.
         Even the ones I talked about for teachers and public police, those were built by private companies and sold to the government institutions.
         They call it<kinesic><desc>expressive hands movement</desc></kinesic>their<quote>secret sauce</quote>, that's why they can't tell us about it.
         It's also private power.
         They're profiting for wielding the authority of the inscrutable.<pause/>
         Now you might think, since<incident><desc>is pr</desc></incident><kinesic><desc>expressive hands movement</desc></kinesic></kinesic> all this stuff is private, there's competition, maybe the free market will solve this problem.
         It won't.
         There's a lot of money to be made in unfairness.
         Also we're note economic rational agents.
         We all are biased.
         We're all racist and bigoted in ways that we wish we weren't,<incident><desc>in wis</desc></incident>in ways that we don't even know.
         We know this though,<incident><desc>ts</desc></incident>in aggregate because sociologists have consistently demonstrated this with these experiments they build, where they send a bunch of applications to jobs out, equally qualified but some have white sounding names and some have black-sounding names,<incident><desc>they ack</desc></incident> it's always disappointing, the results, always.
      </u>
      <u>
         So we are the ones that are biased and we are injecting those biases into the algorithms by choosing what data to collect, like I <pause/> I chose not to think about ramen noodles, I decided it was irrelevant.
         But by<incident><desc>ch</desc></incident>they having the data, trusting the data that's actually picking up <incident><desc>p</desc></incident> on past practices and by choosing the definition of success.
         <shilft new=asc/>How can we expect the algorithms to emerge unscathed? 
         <shift feature="loud" new="f"/>We can't!
         We have to check them. <pause/>
         We've to check them for fairness.
         The good news is we can check them for fairness.
         Algorithms<pause/><kinesic><desc>expressive hands movement</desc></kinesic>can be interrogated and they will tell us the truth every time.
         <kinesic><desc>left hand index finger raised up</desc></kinesic>And we can fix them, we can make them better.
         I call this an algorithmic audit and I'll walk you through it.  
      </u>
      <u>
         First, data integrity check.<pause/>
         <kinesic><desc>expressive hands movement</desc></kinesic>For the recidivism risk algorithm I talked about, a data integrity check would mean we'd have to come to terms with the fact that in the US, whites and blacks smoke pot at the same rate but blacks are far more likely to be arrested, four or five times more likely, depending on the area.
         What is that bias looking like in other crime categories, and how do we account for it?
      </u>
      <u>
         Second,<incident><desc>we th</desc></incident>we should think about the definition of success.
         Audit that.  
         Remember,<shilft new=asc/>with the hiring algorithm?
         We talked about it: someone who stays for four years and<shilft new=asc/>is promoted once?
         Well, that is a successful employee, but it's also an employee that is supported by their culture.
         That said, also it can be quite biased.
         We need to separate these two things.
         We should look to the blind orchestra audition as an example.
         That's where the people auditioning are behind a sheet.
         <incident><desc>What</desc></incident>What I want to think there is the people<incident><desc>wh</desc></incident>who are listining have decided what's important and they've decided what's not important and they're not getting distracted by that.
         When the blind orchestras auditions started, the number of women in orchestras went up by a factor of five.  
      </u>
      <u>
         Next, we have to consider accuracy.
         This is where the value-added for teachers would fail immediately.
         No algorithm is perfect,<shilft new=asc/>of course!
         So we have to consider the errors<pause/>of every algorithm.
         How often are there errors and for whom does this model fail?
         What is the cost of that failure?
      </u>
      <u>
         And finally, we have to consider<pause/>the long-term effects of algorithms, the feedback loops that are engendering.
         And <incident><desc>that</desc></incident> that sounds abstract but imagine if Facebook engineers had considered that<pause/>before they decided<incident><desc>to</desc></incident>to show us only things that our friends<pause/>had posted. 
      </u>
      <u>
         I have two more messages, one for the data scientists out there. 
         Data scientists, we should not be the arbiters of truth.
         We should be translators of ethical discussions that happen in larger society.
         <kinesic who="#Audience"=><desc>applause</desc></kinesic><vocal who="#Audience"><desc>cheer</desc></vocal>
      </u>
      <u>
         And the rest of you, the non-data scientists: this is not a math test, this is a political fight.
         <vocal who="#Audience"><desc>yeah</desc></vocal>
         We need to demand accountability for our algorithmic overlords.
         <vocal who="#Audience"><desc>cheer</desc></vocal><kinesic who="#Audience"=><desc>applause</desc>
         <incident><desc>the</desc></incident><shift feature="sigh" new="giggle"/>the era of big data of blind faith in big data must end.
         Thank you very much.
         <vocal who="#Audience"><desc>cheer</desc></vocal><kinesic who="#Audience"=><desc>applause</desc>
       </u>
    </body>
</text>
</TEI>